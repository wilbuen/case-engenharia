# Wilian Bueno

# üè¢ Case de Engenharia de Dados: Constru√ß√£o de uma Arquitetura Lake House

## üìö Vis√£o Geral

Este projeto faz parte de um case de engenharia de dados que foca na constru√ß√£o de uma **Arquitetura Lake House**. A arquitetura abrange todas as etapas, desde a ingest√£o at√© a transforma√ß√£o e exposi√ß√£o dos dados, garantindo escalabilidade e efici√™ncia no processamento de grandes volumes de informa√ß√µes.

## üìÇ Estrutura do Projeto

O projeto √© organizado em quatro etapas principais:

### 1. **Ingest√£o de Dados (Foi usado de um diretorio no projeto para simular o s3, se fosse ser implementado o s3, seria usado do boto3 para a integra√ß√£o com a aws).**
   - **Fontes de Dados PostgreSQL:** Incluem `condom√≠nios`, `moradores`, `im√≥veis` e `transa√ß√µes`.
   - **Ingest√£o Inicial:** Os dados s√£o extra√≠dos em tempo real de um banco de dados PostgreSQL utilizando **Debezium** e **Kafka**, com checkpoints implementados para garantir toler√¢ncia a falhas. A ingest√£o √© simulada utilizando diret√≥rios locais que funcionam como um bucket S3. Esses dados s√£o armazenados na camada **raw** do Data Lake, no formato **Parquet**. Um script em Python e PySpark √© utilizado para processar os dados dos t√≥picos do Kafka e armazen√°-los na camada **raw**.

### 2. **Transforma√ß√£o de Dados**
   - **Transforma√ß√£o Inicial:** Os dados brutos, que incluem diferentes formatos de datas e valores decimais, s√£o normalizados na camada de transforma√ß√£o. A transforma√ß√£o din√¢mica √© aplicada a campos que contenham as palavras 'data' ou 'valor' nas colunas, lidando com criptografias e outros ajustes necess√°rios.
   - **Transforma√ß√£o Refinada:** Ap√≥s a normaliza√ß√£o, os dados s√£o refinados para gerar insights valiosos para o time de neg√≥cios, com os resultados sendo armazenados na camada **refined** do Data Lake.

### 3. **Processamento de Dados**
   - **Pipelines de Streaming:** Implementa√ß√£o de pipelines de processamento cont√≠nuo utilizando **Spark Structured Streaming**, garantindo a ingest√£o e processamento de dados em tempo real.
   - **Testes Automatizados:** Foram desenvolvidos testes automatizados utilizando **pytest** para verificar e validar todas as etapas do pipeline, assegurando que o processo funcione conforme o esperado.

### 4. **Execu√ß√£o e Testes**
   - **Ambiente Docker:** Todo o processo ocorre em um ambiente Docker, que inclui 5 containers para servi√ßos como Kafka e PostgreSQL. Para iniciar o ambiente Docker, execute os comandos:
     ```bash
     docker-compose build
     docker-compose up -d
     ```
   - **Cria√ß√£o de Conectores:** Para criar os conectores e t√≥picos no Kafka, execute o seguinte script de teste:
     ```bash
     pytest tests/test_0_create_conector_postg_for_kafka.py
     ```
     Este script verifica a conex√£o com o Debezium e cria os t√≥picos necess√°rios para monitorar as tabelas do PostgreSQL. O script de teste j√° inclui uma requisi√ß√£o mockada.

     Com os t√≥picos criados, inicie o streaming dos dados executando:
     ```bash
     python3 extract_with_streaming.py
     ```
     Este comando far√° a ingest√£o dos dados existentes no banco de dados para a camada **raw**.

   - **Teste de Ingest√£o de Dados em Streaming:** 
     Abra um novo terminal e execute o pr√≥ximo teste, que insere dados no PostgreSQL e permite observar a ingest√£o em tempo real:
     ```bash
     pytest tests/test_1_insert_data_to_postgres.py
     ```
     Ser√° poss√≠vel acompanhar a cria√ß√£o dos dados no banco e ver os arquivos Parquet sendo gerados no diret√≥rio `datalake/raw/nometabelas`.

   - **Transforma√ß√£o de Dados:**
     Execute o teste do pipeline de transforma√ß√£o:
     ```bash
     pytest tests/test_2_pipeline.py
     ```

## üõ† Tecnologias Utilizadas

- **Apache Spark** para processamento de dados.
- **Kafka & Debezium** para streaming de dados.
- **PostgreSQL** como banco de dados de origem.
- **Docker** para containeriza√ß√£o do ambiente.
- **pytest** para testes automatizados.
- **Python** como linguagem de programa√ß√£o principal.


## üìù Como seria feita a documenta√ß√£o do design e a implementa√ß√£o da arquitetura e dos pipelines:


### üìÑ 1. **Documenta√ß√£o do Design da Arquitetura**

#### **1.1. Vis√£o Geral da Arquitetura**
- **Diagrama de Arquitetura:** Incluir um diagrama de arquitetura que ilustre as principais camadas do sistema (ingest√£o, processamento, transforma√ß√£o, armazenamento e exposi√ß√£o), detalhando como os dados fluem desde a fonte at√© a camada refinada(Que deve ir ate dataViz, ali foi simulada a entrega na ponta usando um diretorio no projeto).
- **Componentes e Fun√ß√µes:** Descrever cada componente (Kafka, Debezium, Spark, etc.) e sua fun√ß√£o dentro da arquitetura, explicando como eles interagem entre si para garantir o fluxo de dados.

#### **1.2. Camadas de Armazenamento**
- **Camada Raw:** Documentar o formato dos dados na camada bruta (ex: Parquet) e a estrutura dos diret√≥rios no Data Lake. Definindo como os arquivos deveriam estar aqui, que no caso, √© sem necessidade de tratamento.
- **Camada Transform:** Explicar o processo de normaliza√ß√£o e transforma√ß√£o dos dados, incluindo as regras aplicadas para lidar com diferentes formatos de datas e valores. Tamb√©m, disponibilizar dicionario de como √© padronizada essa normaliza√ß√£o, para futuros integrantes no time poderem replicar com facilidade o padr√£o.
- **Camada Refined:** Descrever como os dados s√£o refinados e preparados para an√°lise, incluindo exemplos de queries ou scripts utilizados para gerar os insights.

#### **1.3. Processos de Ingest√£o e Transforma√ß√£o**
- **Ingest√£o de Dados:** Detalhar o processo de ingest√£o de dados em tempo real, explicando o papel do Debezium e Kafka, e como os dados s√£o armazenados no Data Lake. (Importante resaltar que no estado atual do projeto, n√£o estamos identificando o tipo de opera√ß√£o feita no banco, para podermos fazer uma logica para os delete e update nos dados, ou seja, com mais tempo, seria feita essa identifica√ß√£o e divido os dados em diretorios diferentes, para termos esses logs claros dentro da camda raw, posteriomente, unificando eles a partir de logica e tratamento, dentro da camada de transform)
- **Pipelines de Transforma√ß√£o:** Explicar as transforma√ß√µes aplicadas aos dados e os motivos dessas transforma√ß√µes. Incluir pseudoc√≥digo ou exemplos de scripts que ilustram o processo.

### üõ† **2. Documenta√ß√£o dos Pipelines**

#### **2.1. Descri√ß√£o dos Pipelines(N√£o foi implementado Airflow, por conta de maquina, mas uma das sugest√µes para orquestra√ß√£o da camada de transform adiante para o projeto atual, e a implementa√ß√£o do airflow para coletar os dados da raw de forma periodica)**
- **Pipeline de Ingest√£o:** Documentar o funcionamento do pipeline de ingest√£o, desde a configura√ß√£o inicial at√© o processamento dos dados em tempo real. Incluir exemplos de configura√ß√£o do Debezium e t√≥picos Kafka.
- **Pipeline de Transforma√ß√£o:** Detalhar como os dados s√£o processados e transformados, mencionando quaisquer opera√ß√µes complexas, como a normaliza√ß√£o de datas e valores.

#### **2.2. Testes Automatizados**
- **Estrutura de Testes:** Descrever a estrutura de testes criada com `pytest`, incluindo a organiza√ß√£o dos arquivos de testes e o prop√≥sito de cada conjunto de testes (ex: testes de ingest√£o, testes de transforma√ß√£o).
- **Exemplos de Testes:** Fornecer exemplos de como os testes foram implementados, explicando o que cada teste verifica e como ele contribui para a garantia de qualidade do pipeline.

### üìä **3. Monitoramento e Manuten√ß√£o**

#### **3.1. Estrat√©gia de Monitoramento (Nenhum monitoramento organizado foi implementado, portanto, aqui vamos discorrer sobre possibilidades)**
- **Logs e M√©tricas:** Descrever como o monitoramento √© realizado, incluindo a coleta de logs, m√©tricas e alertas. Se ferramentas espec√≠ficas de monitoramento (ex: Prometheus, Grafana) forem utilizadas, detalhar como elas est√£o configuradas.
- **Falhas e Recupera√ß√£o:** Explicar como o sistema lida com falhas e como a toler√¢ncia a falhas √© implementada. Incluir exemplos de situa√ß√µes comuns e como elas s√£o resolvidas.

#### **3.2. Manuten√ß√£o**
- **Atualiza√ß√£o de Pipelines:** Instru√ß√µes sobre como atualizar ou modificar os pipelines existentes, incluindo etapas de valida√ß√£o e teste antes da implementa√ß√£o.
- **Escalabilidade:** Documentar como a arquitetura pode ser escalada para lidar com maiores volumes de dados, incluindo recomenda√ß√µes para ajustes de configura√ß√£o em cada componente.

### üéì **4. Documenta√ß√£o de Usu√°rio**

#### **4.1. Guia de Uso**
- **Execu√ß√£o de Pipelines:** Fornecer um guia passo a passo para executar os pipelines de ingest√£o e transforma√ß√£o, desde a inicializa√ß√£o do Docker at√© a execu√ß√£o dos scripts.
- **Guia de Testes:** Explicar como rodar os testes automatizados, o que esperar como resultado e como interpretar os logs de sa√≠da.
